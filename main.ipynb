{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3282dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as tfs\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from modules.xfeat import XFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e52c47",
   "metadata": {},
   "source": [
    "# XFEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea80ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3349c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from: c:\\Users\\lolke\\Desktop\\fine-tuning lightglue\\modules\\..\\weights\\xfeat.pt\n"
     ]
    }
   ],
   "source": [
    "from modules.xfeat import XFeat\n",
    "xfeat = XFeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04fef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = Image.open(\"dataset/train/002298.jpg\").convert(\"RGB\")\n",
    "img_1_np = np.array(img_1)\n",
    "img_1_tensor = torch.from_numpy(img_1_np)\n",
    "\n",
    "\n",
    "img_1_tensor.shape\n",
    "\n",
    "img_2 = Image.open(\"002328.jpg\").convert(\"RGB\")\n",
    "img_2_np = np.array(img_2)\n",
    "img_2_tensor = torch.from_numpy(img_2_np).permute(2,0,1).unsqueeze(0)\n",
    "img_2_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ae915",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = xfeat.detectAndCompute(img_1_tensor, top_k=100)[0]\n",
    "output_2 = xfeat.detectAndCompute(img_2_tensor, top_k=100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d006b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kpts_1 = output_1['keypoints'].cpu()\n",
    "kpts_2 = output_2['keypoints'].cpu()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_1)\n",
    "plt.scatter(\n",
    "    kpts_1[:,0],\n",
    "    kpts_1[:,1],\n",
    "    c='lime',\n",
    "    s=10,\n",
    ")\n",
    "plt.axis('off')  \n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img_2)\n",
    "plt.scatter(\n",
    "    kpts_2[:,0],\n",
    "    kpts_2[:,1],\n",
    "    c='lime',\n",
    "    s=10,\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf1439",
   "metadata": {},
   "source": [
    "# LIGHTERGLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279870f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LighterGlue weights from: c:\\Users\\lolke\\Desktop\\fine-tuning lightglue\\modules\\..\\weights\\xfeat-lighterglue.pt\n",
      "Loaded LightGlue model\n"
     ]
    }
   ],
   "source": [
    "from modules.lighterglue import LighterGlue\n",
    "\n",
    "matcher = LighterGlue().to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, H1 = img_1.size\n",
    "W2, H2 = img_2.size\n",
    "\n",
    "data = {\n",
    "    'keypoints0': output_1['keypoints'].unsqueeze(0).to(device),\n",
    "    'descriptors0': output_1['descriptors'].unsqueeze(0).to(device),\n",
    "    'image_size0': torch.tensor([[W1, H1]], device=device, dtype=torch.float32),\n",
    "    \n",
    "    'keypoints1': output_2['keypoints'].unsqueeze(0).to(device),\n",
    "    'descriptors1': output_2['descriptors'].unsqueeze(0).to(device),\n",
    "    'image_size1': torch.tensor([[W2, H2]], device=device, dtype=torch.float32),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    matches = matcher(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches['matches'][0].cpu().numpy()   # (M, 2)\n",
    "idx0 = matches[:, 0]   # индексы в первом изображении\n",
    "idx1 = matches[:, 1]   # индексы во втором\n",
    "\n",
    "# 2) Координаты keypoints\n",
    "kpts0 = output_1['keypoints'].cpu().numpy()   # (N0, 2)\n",
    "kpts1 = output_2['keypoints'].cpu().numpy()   # (N1, 2)\n",
    "\n",
    "mkpts0 = kpts0[idx0]   # (M, 2)\n",
    "mkpts1 = kpts1[idx1]   # (M, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, w1, _ = img_1_np.shape\n",
    "h2, w2, _ = img_2_np.shape\n",
    "\n",
    "canvas = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "canvas[:h1, :w1] = img_1_np\n",
    "canvas[:h2, w1:w1+w2] = img_2_np\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(canvas)\n",
    "\n",
    "for (x0, y0), (x1, y1) in zip(mkpts0, mkpts1):\n",
    "    plt.plot([x0, x1 + w1], [y0, y1], c='lime', linewidth=0.8, alpha=0.7)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8752f",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6666f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from modules.dataset.augmentation import AugmentationPipe\n",
    "import os\n",
    "\n",
    "class MatcherDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MatcherDataset\"\"\"\n",
    "    def __init__(self, device, img_dir: Path, img_size: tuple[int, int] =(1280, 1024), difficulty = 0.1, mode='train'):\n",
    "        super(MatcherDataset, self).__init__()\n",
    "        self.img_dir = Path(os.path.join(img_dir,mode))\n",
    "        self.img_paths = sorted(str(img) for img in self.img_dir.iterdir() if img.suffix.lower() in ['.jpg','.png'])\n",
    "        self.img_size = img_size\n",
    "        self.H = img_size[0]\n",
    "        self.W = img_size[1]\n",
    "        self.device = device\n",
    "        self.difficulty = difficulty\n",
    "\n",
    "        \n",
    "        self.gomographPipe = AugmentationPipe(\n",
    "            device=self.device,\n",
    "            img_dir=str(self.img_dir),\n",
    "            warp_resolution=self.img_size,\n",
    "            out_resolution=self.img_size,\n",
    "            sides_crop=0.2,\n",
    "            max_num_imgs=20,\n",
    "            num_test_imgs=1,\n",
    "            photometric=True,\n",
    "            geometric=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_paths[index]).convert('RGB')\n",
    "        img_np = np.array(img)\n",
    "        img_tensor = torch.from_numpy(img_np).permute(2,0,1).unsqueeze(0)\n",
    "\n",
    "        print(img_tensor.shape)\n",
    "\n",
    "        res, ext = self.gomographPipe(img_tensor.to(self.device), self.difficulty, TPS=False)\n",
    "        return img_tensor.squeeze(), res.squeeze().permute(0,2,1), ext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e51293e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 138.96it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 153.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MatcherDataset(device, Path(\"./dataset\"), (1024, 1280), 0.1, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3c6ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 165.65it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 168.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 163.91it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 199.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 144.85it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 177.77it/s]\n"
     ]
    }
   ],
   "source": [
    "def configure_dataloaders(device, dir, img_size, difficulty, batch_size, val_batch_size):\n",
    "    train_dataset = MatcherDataset(device, dir, img_size, difficulty, mode=\"train\")\n",
    "    val_dataset   = MatcherDataset(device, dir, img_size, difficulty, mode=\"val\")\n",
    "    test_dataset  = MatcherDataset(device, dir, img_size, difficulty, mode=\"test\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=len(test_dataset), shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = configure_dataloaders(device, \"dataset\", (1024, 1280), 0.1, 2, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149d41e",
   "metadata": {},
   "source": [
    "## ВСЕ ИЗОБРАЖЕНИЯ ВОЗВРАЩАЮТСЯ С ДАТАСЕТА КАК [B, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f00a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1280])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ---------- конфиг ----------\n",
    "\n",
    "res = dataset[40]\n",
    "img0_t = res[0]        # (3, H_out, W_out)\n",
    "img1_t = res[1]        # (3, H_out, W_out)\n",
    "H_batched = res[2][0]  # (1, 3, 3)\n",
    "mask = res[2][1]       # (1, H_out, W_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9197ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 171.01it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 142.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 169.11it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 176.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synthetic] Found a total of  46  images for training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading train: 100%|██████████| 20/20 [00:00<00:00, 162.95it/s]\n",
      "loading test: 100%|██████████| 1/1 [00:00<00:00, 153.76it/s]\n"
     ]
    }
   ],
   "source": [
    "def configure_dataloaders(device, dir, img_size, difficulty, batch_size, val_batch_size):\n",
    "    train_dataset = MatcherDataset(device, dir, img_size, difficulty, mode=\"train\")\n",
    "    val_dataset   = MatcherDataset(device, dir, img_size, difficulty, mode=\"val\")\n",
    "    test_dataset  = MatcherDataset(device, dir, img_size, difficulty, mode=\"test\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=len(test_dataset), shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader, train_dataset, val_dataset, test_dataset = configure_dataloaders(device, \"dataset\", (1024, 1280), 0.1, 2, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b3f55ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1024, 1280])\n",
      "torch.Size([2, 3, 1024, 1280])\n",
      "torch.Size([2, 1, 3, 3])\n",
      "torch.Size([2, 1, 1024, 1280])\n",
      "2\n",
      "tensor([[1024., 1280.],\n",
      "        [1024., 1280.]])\n",
      "tensor([[1024., 1280.],\n",
      "        [1024., 1280.]])\n",
      "kpts0 torch.Size([2, 1000, 2]) torch.float32\n",
      "desc0 torch.Size([2, 1000, 64]) torch.float32\n",
      "size0 torch.Size([2, 2]) torch.float32\n",
      "kpts1 torch.Size([2, 1000, 2]) torch.float32\n",
      "desc1 torch.Size([2, 1000, 64]) torch.float32\n",
      "size1 torch.Size([2, 2]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "doutput_1 = None\n",
    "output_2 = None\n",
    "Warp = None\n",
    "Mask = None\n",
    "Img = None\n",
    "Res = None\n",
    "\n",
    "W1 = W2 = 1280\n",
    "H1 = H2 = 1024\n",
    "B = 2\n",
    "\n",
    "for img, res, (warp, mask) in train_loader:\n",
    "    print(img.shape, \n",
    "          res.shape, \n",
    "          warp.shape,\n",
    "          mask.shape, sep='\\n')\n",
    "    output_1 = xfeat.detectAndCompute(img, top_k=1000)\n",
    "    output_2 = xfeat.detectAndCompute(res, top_k=1000)\n",
    "    Img = img\n",
    "    Res = res\n",
    "    Warp = warp\n",
    "    Mask = mask\n",
    "    print(len(output_1))\n",
    "    break\n",
    "\n",
    "def __collect_xfeat_to_batch(xfeat_output, W, H, B):\n",
    "    kps = []\n",
    "    dss = []\n",
    "    sizes = []\n",
    "\n",
    "    lengths = [sample['keypoints'].shape[0] for sample in xfeat_output]\n",
    "    M = min(lengths)\n",
    "\n",
    "    for sample in xfeat_output:\n",
    "        k = sample['keypoints']\n",
    "        d = sample['descriptors']\n",
    "\n",
    "        k = k[:M]\n",
    "        d = d[:M]\n",
    "\n",
    "        kps.append(k)\n",
    "        dss.append(d)\n",
    "        sizes.append(torch.tensor([W1, H1], dtype=torch.float32, device=k.device))\n",
    "    \n",
    "    keypoints   = torch.stack(kps, dim=0)   # [B, M, 2]\n",
    "    descriptors = torch.stack(dss, dim=0)   # [B, M, D]\n",
    "    image_size = torch.tensor([[H, W]] * B, dtype=torch.float32, device=img.device)\n",
    "\n",
    "    return keypoints, descriptors, image_size\n",
    "\n",
    "keypoints_0, descriptors_0, image_size_0 = __collect_xfeat_to_batch(output_1, W1, H1, B)\n",
    "keypoints_1, descriptors_1, image_size_1 = __collect_xfeat_to_batch(output_2, W2, H2, B)\n",
    "\n",
    "print(image_size_0)\n",
    "print(image_size_1)\n",
    "\n",
    "data = {\n",
    "    'keypoints0': keypoints_0.to(device),\n",
    "    'descriptors0': descriptors_0.to(device),\n",
    "    'image_size0': image_size_0.to(device),\n",
    "\n",
    "    'keypoints1': keypoints_1.to(device),\n",
    "    'descriptors1': descriptors_1.to(device),\n",
    "    'image_size1': image_size_1,\n",
    "}\n",
    "\n",
    "print(\"kpts0\", data['keypoints0'].shape, data['keypoints0'].dtype)\n",
    "print(\"desc0\", data['descriptors0'].shape, data['descriptors0'].dtype)\n",
    "print(\"size0\", data['image_size0'].shape, data['image_size0'].dtype)\n",
    "\n",
    "print(\"kpts1\", data['keypoints1'].shape, data['keypoints1'].dtype)\n",
    "print(\"desc1\", data['descriptors1'].shape, data['descriptors1'].dtype)\n",
    "print(\"size1\", data['image_size1'].shape, data['image_size1'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7371ec",
   "metadata": {},
   "source": [
    "Что отдает даталоадер torch.Size([8, 3, 1024, 1280]) torch.Size([8, 3, 1280, 1024]) torch.Size([8, 1, 3, 3]) torch.Size([8, 1, 1280, 1024])\n",
    "\n",
    " output_1 = xfeat.detectAndCompute(img, top_k=4024)\n",
    " отдает список из 8 элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 1280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_gt(kps0, kps1, H, mask, pipe):\n",
    "    B, N, _ = kps0.shape\n",
    "    device = kps0.device\n",
    "\n",
    "    W_orig, H_orig = pipe.dims\n",
    "    sides_crop = pipe.sides_crop\n",
    "    print(W_orig, H_orig)\n",
    "\n",
    "compute_gt(keypoints_0, keypoints_1, Warp, Mask, train_dataset.gomographPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30f94221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARP:  torch.Size([2, 1, 3, 3])\n",
      "MASK:  torch.Size([2, 1, 1280, 1024])\n",
      "IMG:  torch.Size([2, 3, 1024, 1280])\n",
      "RES:  torch.Size([2, 3, 1280, 1024])\n"
     ]
    }
   ],
   "source": [
    "#keypoints_0, descriptors_0, image_size_0\n",
    "#keypoints_1, descriptors_1, image_size_1\n",
    "\n",
    "#Warp\n",
    "#Mask\n",
    "\n",
    "#Img\n",
    "#Res\n",
    "print(\"WARP: \", Warp.shape)\n",
    "print(\"MASK: \", Mask.shape)\n",
    "\n",
    "print(\"IMG: \", Img.shape)\n",
    "print(\"RES: \", Res.shape)\n",
    "\n",
    "def __compute_gt(keypoints_0, keypoints_1, warp, mask, px_thresh, augmentor):\n",
    "    B = keypoints_0.shape[0]\n",
    "    gt_assignment_list = []\n",
    "    gt_matches0_list = []\n",
    "    gt_matches1_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        kps0 = keypoints_0[b]        # [M0, 2]\n",
    "        kps1 = keypoints_1[b]        # [N0, 2]\n",
    "        H = warp[b].squeeze()        # [3, 3]\n",
    "        V = mask[b].squeeze()        # [H, W]\n",
    "        if V.ndim == 3:\n",
    "            V = V.squeeze(0)\n",
    "\n",
    "        kps0_proj = augmentor.warp_points(H, kps0)  # [M0, 2]\n",
    "\n",
    "        V_H, V_W = V.shape\n",
    "        x = kps0_proj[:, 0]\n",
    "        y = kps0_proj[:, 1]\n",
    "        in_bounds = (x >= 0) & (x < V_W) & (y >= 0) & (y < V_H)\n",
    "        valid = in_bounds.clone()\n",
    "        if in_bounds.any():\n",
    "            xi = x[in_bounds].long()\n",
    "            yi = y[in_bounds].long()\n",
    "            mask_valid = V[yi, xi] > 0.5  # Проверяем только внутренние точки\n",
    "            valid[in_bounds] = mask_valid\n",
    "\n",
    "        kps0_proj_valid = kps0_proj[valid]\n",
    "        idx0_valid = torch.nonzero(valid, as_tuple=False).squeeze(1)\n",
    "\n",
    "        M0 = kps0.shape[0]\n",
    "        N0 = kps1.shape[0]\n",
    "        gt_matches0 = torch.full((M0,), -1, device=kps0.device, dtype=torch.long)\n",
    "        gt_matches1 = torch.full((N0,), -1, device=kps1.device, dtype=torch.long)\n",
    "        gt_assignment = torch.zeros((M0, N0), device=kps0.device, dtype=torch.float32)\n",
    "\n",
    "        if kps0_proj_valid.numel() == 0 or N0 == 0:\n",
    "            gt_assignment_list.append(gt_assignment)\n",
    "            gt_matches0_list.append(gt_matches0)\n",
    "            gt_matches1_list.append(gt_matches1)\n",
    "            continue\n",
    "\n",
    "        # ✅ ФИКС: Находим лучшие matches (не первые)\n",
    "        dx = kps0_proj_valid[:, None, :] - kps1[None, :, :]\n",
    "        d2 = (dx ** 2).sum(-1)\n",
    "        min_d2, nn_j = d2.min(dim=1)\n",
    "\n",
    "        good = min_d2.sqrt() < px_thresh\n",
    "        if good.sum() == 0:\n",
    "            gt_assignment_list.append(gt_assignment)\n",
    "            gt_matches0_list.append(gt_matches0)\n",
    "            gt_matches1_list.append(gt_matches1)\n",
    "            continue\n",
    "\n",
    "        k0_good = idx0_valid[good]       # [G]\n",
    "        k1_good = nn_j[good]             # [G]\n",
    "        distances = min_d2[good].sqrt()  # [G]\n",
    "\n",
    "        # Оставляем только ближайшие matches для каждой k1\n",
    "        if len(k1_good) > 0:\n",
    "            best_mask = torch.zeros_like(k1_good, dtype=torch.bool)\n",
    "            unique_k1, inverse_indices = torch.unique(k1_good, return_inverse=True)\n",
    "            \n",
    "            for uniq_idx in range(len(unique_k1)):\n",
    "                matches_for_this_k1 = (inverse_indices == uniq_idx)\n",
    "                if matches_for_this_k1.sum() > 0:\n",
    "                    best_idx_in_group = distances[matches_for_this_k1].argmin()\n",
    "                    global_idx = torch.nonzero(matches_for_this_k1, as_tuple=False)[best_idx_in_group].item()\n",
    "                    best_mask[global_idx] = True\n",
    "            \n",
    "            k0_good = k0_good[best_mask]\n",
    "            k1_good = k1_good[best_mask]\n",
    "\n",
    "        gt_matches0[k0_good] = k1_good\n",
    "        gt_matches1[k1_good] = k0_good\n",
    "        gt_assignment[k0_good, k1_good] = 1.0\n",
    "\n",
    "        gt_assignment_list.append(gt_assignment)\n",
    "        gt_matches0_list.append(gt_matches0)\n",
    "        gt_matches1_list.append(gt_matches1)\n",
    "\n",
    "    gt = {\n",
    "        \"gt_assignment\": torch.stack(gt_assignment_list, dim=0),\n",
    "        \"gt_matches0\": torch.stack(gt_matches0_list, dim=0),\n",
    "        \"gt_matches1\": torch.stack(gt_matches1_list, dim=0),\n",
    "    }\n",
    "    return gt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gt = __compute_gt(keypoints_0, keypoints_1, Warp, Mask, 1, train_dataset.gomographPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01190d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corners1: tensor([[-430.7583,  216.0655],\n",
      "        [1043.7958,  -94.3170],\n",
      "        [ 200.6894, 1192.2239],\n",
      "        [1260.1873,  689.1003]])\n"
     ]
    }
   ],
   "source": [
    "H_b = Warp[0].squeeze().cpu()\n",
    "H0, W0 = Img.shape[-2:]\n",
    "corners0 = torch.tensor([[0,0], [W0-1,0], [0,H0-1], [W0-1,H0-1]], dtype=torch.float32)\n",
    "ones = torch.ones((4,1))\n",
    "c0_h = torch.cat([corners0, ones], 1)\n",
    "c1_h = (H_b @ c0_h.T).T\n",
    "c1 = c1_h[:, :2] / c1_h[:, 2:3]\n",
    "print(\"corners1:\", c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8444991",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (1280) at non-singleton dimension 0.  Target sizes: [1024, 1024, 3].  Tensor sizes: [1280, 1024, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m canvas = torch.zeros((H, W0 + W1, \u001b[32m3\u001b[39m), dtype=img0_vis.dtype)\n\u001b[32m     35\u001b[39m canvas[:, :W0] = img0_vis\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mcanvas\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW0\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m = img1_vis\n\u001b[32m     38\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m     39\u001b[39m plt.imshow(canvas)\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (1024) must match the existing size (1280) at non-singleton dimension 0.  Target sizes: [1024, 1024, 3].  Tensor sizes: [1280, 1024, 3]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "b = 0  # индекс в батче, который хотим посмотреть\n",
    "\n",
    "# изображения из батча (Img, Res: [B, C, H, W] в [0,1] или [0,255])\n",
    "img0_b = Img[b].detach().cpu()\n",
    "img1_b = Res[b].detach().cpu()\n",
    "\n",
    "# keypoints и GT-матчи для этого элемента\n",
    "kps0_b = keypoints_0[b].detach().cpu()                  # [M, 2]\n",
    "kps1_b = keypoints_1[b].detach().cpu()                  # [N, 2]\n",
    "matches0_b = gt[\"gt_matches0\"][b].detach().cpu()        # [M]\n",
    "\n",
    "# оставляем только действительно заматченные точки\n",
    "valid_m = matches0_b >= 0\n",
    "idx0 = torch.nonzero(valid_m, as_tuple=False).squeeze(1)  # индексы в kps0_b\n",
    "idx1 = matches0_b[valid_m]                                # соответствующие индексы в kps1_b\n",
    "\n",
    "pts0 = kps0_b[idx0]   # [K, 2]\n",
    "pts1 = kps1_b[idx1]   # [K, 2]\n",
    "\n",
    "def to_hwc(img):\n",
    "    # [C, H, W] -> [H, W, C]\n",
    "    if img.ndim == 3:\n",
    "        return img.permute(1, 2, 0).float() / (255.0 if img.max() > 1.5 else 1.0)\n",
    "    return img\n",
    "\n",
    "img0_vis = to_hwc(img0_b)\n",
    "img1_vis = to_hwc(img1_b)\n",
    "\n",
    "# склеиваем изображения по ширине\n",
    "H, W0, _ = img0_vis.shape\n",
    "W1 = img1_vis.shape[1]\n",
    "canvas = torch.zeros((H, W0 + W1, 3), dtype=img0_vis.dtype)\n",
    "canvas[:, :W0] = img0_vis\n",
    "canvas[:, W0:] = img1_vis\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(canvas)\n",
    "\n",
    "# точки из первой картинки как есть,\n",
    "# точки из второй — со сдвигом по x на ширину первой\n",
    "offset = torch.tensor([W0, 0.0])\n",
    "pts0_plot = pts0\n",
    "pts1_plot = pts1 + offset\n",
    "\n",
    "# рисуем линии соответствий\n",
    "for p0, p1 in zip(pts0_plot, pts1_plot):\n",
    "    plt.plot([p0[0], p1[0]], [p0[1], p1[1]], 'r-', linewidth=0.4, alpha=0.7)\n",
    "\n",
    "# сами ключевые точки\n",
    "plt.scatter(pts0_plot[:, 0], pts0_plot[:, 1], s=5, c='lime')\n",
    "plt.scatter(pts1_plot[:, 0], pts1_plot[:, 1], s=5, c='yellow')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('GT correspondences via homography')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
